So the first thing we're going to do is we're going to create some virtual machines that are going to
end up behind a load balancer.
But when we do that, we're going to this time choose the availability options.
So I'm going to go in and create these machines in what's called an availability set.
And the first time I do this, I have to create a new availability set.
And this will set one now a false domain is basically a physical separation of these virtual machines
such that no hardware failure is going to disrupt both of them.
So in a power supply or a a physical rack or some type of hub or switch, so falta means is default
is to but you can have up to three.
And basically the virtual machines will be distributed between these physically separated devices.
Updater Means is more of a logistical concept where Microsoft will roll out updates to the azure fabric
and it'll do it one at a time.
And if you're going to distribute your virtual machines across those, then there's going to be no planned
updates that are passing across multiple virtual machines at the same time.
So I'm going to be able to set one and I'm just going to accept the defaults.
So this won't be two sides.
Skip all of this and get right to the review.
It did pick.
I just wanted to check it did pick my existing virtual network, which is great.
So I'm going to create and I'm going to do this a second time.
And in when they return from this video, I'll have to virtual machines in an availability set.
Now, while we're waiting for those virtual machines to be created, let's go and create the load balancer
and hopefully by the time we get to that point, then those machines will be up and running.
I mean, the research group, I'm going to say and and I'm going to look for load balancer.
Now, there are actually a couple of different types of load balancers in Azure that are supported by
Microsoft.
Load balancer is the first one another's called an application gateway.
But we'll talk about that in another video.
So this is the load balancer I'm going to just click create.
Now, the basic load balancer is free.
If you want to upgrade, there is a four standard load balancer.
There's a little cost to it.
So we're going to create this little balance.
I'm going to use the same resource group that I created the virtual machines in.
We do have to give this a name.
I'm going to call this new Elby.
The region is important.
It has to be running in the same region as the virtual machines.
So we created these VMS in East USA.
And so load balancer needs to be in that same region as well.
Remember, the diagram we showed had both a public and an internal load balancer.
This is going to be a public load balancer.
What that means is it has a public IP address and that needs a name.
I'm going to call it public IP.
If it was an internal bouncer, it would not have a public IP address.
All right.
Scrolling down, we're just going to set the defaults.
It's going to be dynamically assigned, no IP addresses, click review and create.
So new LP is what we're going to be looking for.
This should deploy relatively quickly.
Unlike an application gateway, this is not actually an instance of a like an application.
Gateways are running instances like a mini virtual machine doing its work.
A little bouncer's really just some settings in one of the internal database tables at Microsoft.
So like in seconds the load balancer was created.
So I go into it and it's my Vi√±a Elby in E to us to now there are three essential elements of a load
balancer and it's the same with the application gateway as well.
There is the front end.
So in this case, the front end configuration, there's the back end.
These are the servers, the groups of servers that are handling the traffic and there's the rules.
So how does the traffic travel from the front end to the back end?
And this type of bouncer can handle multiple public IP addresses, multiple front ends, and it can
handle multiple pools of servers on the back end and multiple rules.
And so you can have traffic coming into three or four domains going to three or four sets of servers
with different rules of how traffic travels.
We can also have health probes, which we mentioned is how the load balancer knows that the backend
pools are operating correctly and will remove the servers that are not.
Now, if I go to the front end IP, I can just see public IP.
There's nothing much to it.
Let's go right into the back end.
So we're going to create our first pool.
So I guess the pool is a group of servers that are going to handle traffic as one.
This is the first pool I'm going to put this onto.
I created a virtual network for this IP for now.
As of right now, there's no there's no servers in this pool.
But I can basically choose the virtual machines that I want to put into it.
So I want to add virtual machines.
And you can only touch screen machines that have a basic skew, public IP configuration or no public.
I think in the configuration they must be in the same availability set and on the same network.
So I'm going to say add and these are the two machines that we just created at the beginning.
We can see the IP addresses there in an availability set.
These are who was going to handle the traffic.
One thing I haven't done is I haven't logged into these machines and set up Ayaz and set up a Web server,
made them identical, which is going to have to assume that these are able to handle Web traffic.
I'm going to say add.
And so now it goes off and it creates this back in pool attached to this load balancer.
And we can see that it's there, it's running now, the last part that's missing, of course, is how
traffic travels from the front end to the back end pull.
So that's called a load balancing rule.
And I'm going to say at.
So lability will distribute some traffic that was sent to an IP address across a backyard pool, for
instance, it says we have to create the problem first.
OK, well, so we're going to go into the health probes.
We're going to say.
And now what health probe is going to do is it's going to check.
A particular Tsipi location or HTP address and see that it's healthy, so I switched over to HTP and
it's going to check that Port 80, which is the Web port, the root of that is going to have to return
a two hundred OC status every five seconds.
And if after 10 seconds it doesn't return a healthy status, then it's considered a unhealthy server.
So if I create this probe here, this is going to this is the health probe, essentially.
So that was successful.
Now we should really go into load balancing rules, it's still in an updating state.
All right, so we go back to the low bouncing world, as we can say, and give it a name, so new rule
and we are going to choose the load balancer front end.
Don't know why that says no TCP Port 80.
So it's going to basically pass in Web traffic over Port A-T.
The pool it'll send it to is my first pool.
It's going to use the health probe to ensure that these servers are healthy and will live out the time
out in terms of the TTP open connection.
For four minutes only am I going to say, OK, now this will get created, it'll take unfortunately
10 or 15 minutes.
Every time I do something, it takes 10 or 15 minutes to update.
But at the end of this, we will have a working load balancer.
Now, like I said, I haven't set up the back and pull to respond over Port 80.
I mean, we should we should try that and then we can test this traffic coming through a load balancer
to one or the other server.
So I'll pause the video and do that.
Of course, I forgot to open a hole in the firewall to allow for any traffic through, so you have to
go to the individual virtual machines, VM, one of Vimto under networking, and add a rule for Port
80 to allow traffic through.
And once you do that, you can go to the IP address of the load balancer.
So let's go back to the load balancer.
And there's the IP address and we can see the little bouncer is working, so let's put a long video
and I apologize for that.
But what we've done is created a couple of virtual machines, load balancer in front of them, the traffic
rules, the health probe installed eyes on each of them, opened up on the firewall.
And we have a working load balancer.
Now, the more I refresh this, we're going to be sent between VM one VM to VM, one VM to randomly
like.
We can't be assure which one we're being sent to.
If we modify these pages, we might be able to see one come up more often, the other whatever.
But the load balancers taking care of distributing traffic evenly to the two VMS behind it.
