<v Instructor>And so a related concept</v>
to scaling is this concept, elasticity, right?
You'll hear, especially with AWS,
they tend to have, everything is elastic.
They have elastic databases,
and elastic computes, and elastic storage.
What is elastic, right?
The elastic element is that the system can
automatically grow and shrink based
on the application's demand, so the rubber band aspect.
So we had that graph when we said about scaling,
where we have a maximum capacity,
where you have an increasing user demand over time,
and that failure point, OK?
But if your application had, it was elastic,
then the capacity can grow, and so as your demand grows,
capacity raises to meet it, right?
And then when the demand eventually shrinks,
as it normally does, then the capacity shrinks back down.
That is an elastic system, auto scaling.
And you'll see the capacity is sort of staggered.
It's got steps, not smooth, because typically,
when you have four servers and then you add a fifth server,
the growth in capacity is not smooth.
You've just increased your capacity 20%.
And then you add a couple more servers,
you've increased your capacity another 20%.
So capacity tends to come in bursts, essentially.
And so this elastic thing, the requirement
for that is the system can detect when it's getting busy
and add more resources when it is.
And because we're in the Cloud, it's cheaper.
You'll end up saving a lot of money,
because your system is elastic.
Now, it's important to understand
where these failure points can happen, right?
So, when we're talking about availability
and we're saying, why isn't it 100?
Can't you just install an application
into a server and it runs, why?
Well, the thing is, we're living in the world of computers
and hardware fails, right?
You have a home computer.
Sometimes your power supply dies.
Sometimes your hard drive dies.
Sometimes the memory goes wonky.
That just happens, hardware erodes over time.
Those connections get a thing.
There's a little bit of corrosion going on.
Dust gets in there, hardware fails.
We live also in this world where, you know,
your regional elements are out of your control, right?
There could be a storm, a flood, power outage.
I'm old enough to remember when the whole East Coast
of the U.S. and Canada were down, no electricity for a day.
And you know, that stuff happens,
and it's hard to keep your data center running
when everything else around you is out of power.
So your batteries only last for so long,
and the gas and stuff lasts for so long,
but if the Internet's down, the Internet's down.
We also suffer iterative outages,
where traffic starts to fail, packets are being dropped,
routers have gone bad, it happens.
Now, we're also at the mercy of these cloud providers,
for their own deployments and their own software, right?
We're running our own, we're running our software,
but they're also running their software, so it does happen
that Microsoft Azure will deploy something to a region,
and that was just some unexpected thing happened,
a bad deployment, they have to roll it back.
Don't forget, we live in a world of security,
and we have to patch Windows sometimes.
We have to patch our software.
Microsoft has to patch their own Azure operating system.
Things need updates,
and so that requires a reboot sometimes,
and that requires a downtime.
So downtime happens in a lot of unplanned ways,
and also even in planned ways, which are the updates.
To get that high availability, if you want that 99.999%,
what about your own code?
What about your own app?
You know, how are you going to deploy version two
of your app without taking down version one?
Well, there are technologies for that, like service fabric.
You can deploy version two of something,
and it'll just switch over with no downtime for users.
But that's something that's intentional, thoughtful,
planned in advance, it's not by default.
How can you update the database, add a column,
remove a column, change the structure of something
while your application is still running?
All those sorts of things.
Don't forget we live in a dangerous world,
where denial-of-service attacks are a thing.
People can go to your application
and send a million visits at the same time to it,
and all of your infrastructure,
even if you're elastic and scalable, gets overwhelmed.
Your load balancer gets overwhelmed,
the incoming stuff gets overwhelmed,
and existing customers suffer.
Also, just mistakes happen, right?
You can accidentally check in some code that's not complete,
missing a file, and hit that deployment switch,
and suddenly you're on production.
It's happened to all of us.
It'll continue to happen.
I remember, even with AWS, happened once
where somebody was entering a command line.
They were trying to remove an item from a routing table,
but they didn't specify the IP address range properly.
They made a typo, if you will, hit Enter,
and the AWS engineer wiped out the routing table.
And AWS' Eastern region was down for 6 hours
because of a human error.
It happens.
So I'm trying to bring this home here.
Azure goes down, AWS goes down, Google goes down,
every major cloud provider has their moments
where they have some sort of operation,
hopefully not frequent.
But you need to design your applications
to keep running during these things.
I mean, I guess if it's a priority to you, you need to.
How do you design your app if,
as your Eastern region goes down,
how do your app continue to run?
Well, there are ways to do that.
I'm not going to cover that during this intro here,
we're still in the intro section to this course,
but attaining high availability,
it's an architectural problem.
There are solutions for it.
All the time, these companies, including Azure,
coming out with improvements.
So Azure front-door service was just released,
last couple of months.
That's an improvement to the way that these things happen
and can be managed on a regional level.
Related to these things of scalability
and keeping availability, keeping it running,
what happens if it does go down, right?
You got keeping it running on one side.
the flip side of the same coin is being able
to recover from failure.
And the two questions that are the key questions
for recovering is: how is an acceptable amount of downtime?
Okay, so let's say you're down for an hour,
and it was a disaster, is that acceptable?
And let's say it's not, let's say you're like,
"No, we need to be up and running within five minutes."
Well, how much data would you be willing to lose?
To get that up and running in five minutes,
you need to have your data backed up
and stored in alternative locations,
and accessible from those locations.
So how often are you doing backups?
You doing backups every five minutes?
Are you doing backups constantly?
Is it a constant synchronization?
And so these things are part of disaster recovery.
