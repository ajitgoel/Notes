 
 WEBVTT 
 Okay. 
 Hello clunkers. 
 And welcome to this lecture. 
 This lecture is on s. 
 And as I mentioned in the last lecture S3 is one of the oldest IWM services. 
 And it makes up an awful lot of the certified Solutions Architect associate exam. 
 So what is S3 or S3. 
 Basically stands for simple storage service. 
 So it's three S's. 
 That's why it's called S3 and it provides developers and I.T. teams with secure durable highly scalable 
 object storage and S3 is easy to use. 
 The simple web services interface to store and retrieve any amount of data from anywhere on the web. 
 So what does that actually mean. 
 Well basically S3 is a safe place to store your files and that's all it means by objects. 
 So when we say object storage we're talking about things flat files basically so word documents pitches 
 movies et cetera et cetera. 
 So S S3 is basically a safe place to store your files. 
 It's object based storage and the data is spread across multiple devices and multiple facilities. 
 And so the basics of S3 are as follows its object base it allows you to upload files and your files 
 can be anywhere from zero bytes all the way up to five terabytes. 
 There is unlimited storage and files are stored in these things called buckets. 
 Now when I first heard about buckets it sounded a bit strange but a bucket is basically just a folder. 
 That's all it is. 
 So it's a folder in which to put your files. 
 Now the thing about buckets is that S3 is essentially a universal namespace. 
 So buckets must be unique globally. 
 And the reason for that is when you create a bucket it's going to get a web address. 
 So it's going to look exactly like this. 
 It's going to say S3 and then hyphen and then the region in which the bucket is then dot Amazon NWS 
 dot com and then forward slash the bucket name. 
 So a cloud guru. 
 And so that's why the buckets need to be unique to people cannot have the same name because there's 
 only one web address available so you have to make sure your bucket names are unique and when you upload 
 a file to S3 you're going to receive a Haitian GDP 200 code and that's basically just saying that the 
 upload has been successful so that will come back to your browser and that can be a popular exam topic 
 as well. 
 Moving on I keep saying that S3 is object based and just to think of objects as files and objects basically 
 consist of the following. 
 So we've got a key and this is simply the name of the object. 
 So the file name of the object we then have the value and this is simply the data that is made up of 
 a sequence of bytes. 
 So it's just the data we then have a version I.D. And this is important for versioning and we're going 
 to have a lab on versioning but effectively S3 allows you to have multiple versions of your file so 
 you get a version one which says hello cloud gurus and then you could go in and upload a file with the 
 same name and change the file so it says hello cloud gurus welcome to version 2 or something like that 
 and then you'll have both versions of the object in S3 and you'll be able to do version control so you 
 could go back to a previous version. 
 So S3 allows you to do versioning and like I said we're going to have a lab on that. 
 We also then have metadata. 
 If you've never heard the term metadata before it just simply means data about data says data about 
 the data that your storing. 
 You may say okay this belongs this object belongs to the finance department. 
 This is our payroll spreadsheet something like that and then it has a bunch of different sub resources 
 we have access control lists and we also have torrents. 
 We are going to cover off the access control list and bucket policies coming up. 
 So now that you know what S3 is and that it's just a safe place to store your objects in the cloud will 
 you really have to understand the data consistency model for S3 going into your exam so how does S3 
 keep your data consistent. 
 Or basically S3 has two things it has read after right to consistency for puts of new objects and then 
 it has eventual consistency for overwrite puts and deletes. 
 Now I know what you're thinking. 
 What does that mean. 
 Well the first one read after right consistency for puts of new objects. 
 All that means is is if you upload a file to S3 you are able to read it immediately so you are able 
 to read it straight after writing to it. 
 And that's basically you're just doing a put of that object into S3. 
 Now if you get to go in and basically update that object or delete that object so overwrite that object 
 then it's only going to be eventual consistency so let's say we've got version 1 of a file and then 
 we upload Version 2 and you immediately try and read that object. 
 Basically what's going to happen is you might get version one or you might get version two but if you 
 wait like a second you're always going to get version two. 
 So there's eventual consistency for overwrite puts and delete. 
 So only when you're overwriting a file or you're deleting a file eventually it's going to be consistent. 
 That's all that means. 
 So you have read after right consistency for new objects and you have eventual cases consistency for 
 overwrite puts and deletes. 
 So in other words if you write a new file and read it immediately afterwards you'll be able to view 
 that data. 
 If you update an existing file or delete a file and read it immediately you may get the older version 
 or you may not. 
 Basically the changes can take a little bit of time to propagate. 
 So what other S3 guarantees what does Amazon guarantee from S3. 
 It's built for ninety nine point nine nine per cent availability and Amazon will guarantee ninety nine 
 point nine per cent availability and Amazon guarantees ninety nine point nine nine nine nine. 
 I'm not going to say it but basically this is referred to as eleven nines because there's eleven nines 
 there durability for S3. 
 So just remember eleven nines. 
 And what does that mean. 
 Well that means if you put your object up in S3 it's not it's very very unlikely that it's going to 
 be lost. 
 It's only going to be lost one in however many nines that s so it basically guarantees the durability 
 of your object and then S3 has this following features so it's got tiered storage so it's got different 
 storage tiers and this is a fundamentally important topic going into your certified solutions architect 
 exam. 
 This is worth like four or five points you need to know the different tiers and we'll come onto that 
 in ace in the next slide. 
 We also have lifecycle management so you can actually go through and move your objects around to different 
 storage tiers so you could say hey when this file is 30 days old move it to this tier when it's 90 days 
 old archive it off to Glacier and I'll cover what closure is in a second. 
 It also allows you to do versioning so you can have multiple versions of objects in your S3 Buckets 
 you're allowed you're able to encrypt your objects you're allowed to encrypt them at rest and there's 
 different encryption mechanisms which will cover off in another lecture. 
 Again this is really important you're able to use multi factor authentication for deleting objects. 
 So when you turn this on if someone goes in to delete an object they're going to need two factor authentication 
 in which to do this that they're going to need to use something like Google Authenticator and then you 
 secure your data using access control lists and bucket policies and we're going to cover those off later 
 on in this section of the course as well. 
 So I talked about the different storage classes and how important they are to you know passing your 
 exams so let's cover them off now so S3 standard. 
 This is the one with ninety nine point nine nine per cent availability and 11 nines durability and stored 
 redundant across multiple devices in multiple facilities and is designed to sustain the loss of two 
 facilities concurrently. 
 So it is really really highly available and highly durable. 
 You then have S3 infrequently accessed and this is basically for data that has access less frequently 
 but requires rapid access when you need it so you get a lowest storage fee then S3 but you are charged 
 a retrieval fee so if you've got data that you need to be able to instantly access but you pretty much 
 don't you know you don't use that data regularly maybe only need to check it at the end of every month 
 then you'd want S3 infrequently access you then also have S3 one zone infrequently access and this is 
 when you want a really low cost option for your infrequently access data and you don't even need you 
 don't have to worry about multiple Availability Zones so it is literally just stored in one availability 
 zone and it's infrequently access but you still need to be able to access that data instantly and then 
 reinvent. 
 In 2018 Amazon released S3 intelligent tearing. 
 This is a really cool technology this is using machine learning and basically what it does is it looks 
 at how often you use your objects and then it will move your objects around the different storage classes 
 based on what it's learnt so we'll move it from S3 standard to S3 infrequently access because it knows 
 that you don't access those files so that's S3 intelligent tearing those are the four storage classes 
 we then have glacier and glacier comes in two flavors. 
 So we've got normal S3 glacier and then we've got S3 glacier deep archive and so glacier is basically 
 for data archiving so if you want to archive off your data maybe you don't need it. 
 Maybe you have to hold onto it for seven years because of some federal regulation. 
 You would be using glacier now you can restore any amount of data and it's really super super cheap 
 and then you retrieval times are configurable from minutes to hours. 
 Now a glacier deep archive that's the lowest storage class the lowest cost storage class that you can 
 buy but your retrieval time is going to be 12 hours. 
 So if you want to get the data back using deep archive you you put in a request and it will come back 
 12 hours later. 
 So those are the six different storage tiers and you will need to remember those going into your exam 
 and so here is a table that compares the different storage. 
 Here's the thing that you should really note is the first byte latency that's how quickly you'll be 
 able to access your data. 
 So it's milliseconds for everything apart from S3 glacier and glacier deep archive so it can be minutes 
 or hours. 
 Or it could be up to 12 hours. 
 If you're using deep archives so bear that in mind as well. 
 So how are you going to be billed for S3. 
 Well you're charged in the following ways you're charged in terms of storage. 
 So the more you store in S3 the more you're going to be billed. 
 You also charge on the number of requests. 
 If you're making a lot of requests to those objects it's going to be more expensive. 
 You then get charge on the storage management pricing so this is the different tiers that are available. 
 You then also get charged on data transfer pricing and then you also get charged for transfer acceleration 
 and cross region replication. 
 And don't worry we're going to cover exactly what those two things are right now. 
 So what is cross region replication. 
 Well let's say you've got a bucket and it's in us e one and you want to automatically replicate your 
 objects to another bucket that's in let's say in Sydney and you want to do that for high availability 
 as well as you know disaster recovery. 
 So what will happen is as soon as you upload an object to your bucket in us e 1 and you've got cross 
 region replication turned on those objects will automatically be replicated over to your bucket. 
 In Sydney we're going to have a lab on that coming up. 
 We then also have S3 transfer acceleration. 
 This enables fast easy and secure transfers of files over long distances between year end users and 
 an S3 bucket. 
 And basically it's taking advantage of Amazon's cloud fronts globally distributed edge locations and 
 then the data arrives at an entry location is routed to Amazon S3 over and optimize optimized network 
 Path and essentially what they're doing is they're using the Amazon backbone network. 
 So let's have a look at how this works got our users all around the world and then we've got our edge 
 locations. 
 And essentially if you turn on transfer acceleration they are uploading those their files to the edge 
 locations rather than to the S3 bucket itself. 
 And then this goes over Amazon's backbone networks Amazon's own network. 
 Straight back to where your bucket is located and can really speed up your users upload times. 
 And we're going to have a lab on that and we're gonna see how it works from all the different locations 
 in the world coming up in this section of the course. 
 We've done really really well. 
 We're at the end of this lecture. 
 Let's look at my exam tip so just remember that S3 is object base. 
 It allows you to upload files. 
 Those files can be 0 bytes to 5 terabytes in size. 
 There's unlimited storage and your files are stored in these things called buckets and a bucket is basically 
 just a folder in the cloud. 
 Now each bucket has to have its own unique name. 
 And that is to say that S3 is a universal namespace you must have a unique name for your buckets and 
 your link when you create a bucket. 
 It's going to generate a heap TDB s link and it's going to look like this. 
 It's going to be S3 hyphen EU w hyphen once S3 and then the region and then dot Amazon NWS dot com forward 
 slash and then your bucket name. 
 Moving on. 
 Just remember the S3 because it is object base it's not suitable to install an operating system or a 
 database on. 
 So it's not you know that's not that type of storage that you will want book based storage. 
 This is object based storage. 
 So essentially it's only used to store files you're not going to install an operating system on S3 and 
 you're not going to use it to host host a database. 
 And when you successfully upload an object to S3 you're going to get a heap TDB 200 status code. 
 Also remember that you can protect your objects by turning on multi factor authentication delete. 
 That's really important to remember going into your exams so that's how you can protect your objects 
 so that somebody doesn't go in and accidentally delete them they'll need to do multi factor authentication. 
 Remember the key fundamentals of S3 So what makes up an S3 object. 
 Well we have the key and this is simply the name of the object. 
 We then have the value and this is simply the data and is made up of a sequence of bytes. 
 We then have the virgin I.D.. 
 This is important for versioning. 
 We have metadata which is data about the data that you're storing. 
 We then have some sub resources within here. 
 So we've got access control lists and torrents and access control list. 
 Basically it's the permissions that of that object and you can lock each object down individually so 
 you can do it at a bucket level and you can also do it at an object level as well. 
 We're going to cover that off later on in the course. 
 Now this is really important to remember the consistency models it's read after right consistency for 
 puts of new objects. 
 So as soon as you create a new object you'll be able to read that object immediately but if you update 
 an object or delete an object and you try and read it immediately you're going to only get eventual 
 consistency so you might get the older object or you might still see the deleted file. 
 But if you wait about a second it's going to be consistent so you get eventual consistency and pretty 
 much the most important thing to remember going into your exam are the different storage classes we've 
 got S3 standard. 
 This is ninety nine point nine nine percent availability. 
 11 nines durability with an amp S3 infrequently access. 
 This is basically where you need you know you're going to pay a lower fee than standard S3 but you need 
 instant access and you are charged a retrieval fee as well. 
 This we then have S3 one zone I.A. or one zone infrequently access. 
 This is where you want a lower cost option for your infrequently access data but you don't require multiple 
 Availability Zones and just so you guys know this used to be called r s or reduce redundancy storage. 
 It is being phased out. 
 Now that terminology but if you do see r r s or reduce redundant C storage in your exam. 
 Just think of it as S3 one Zune II. 
 I literally just did the exam yesterday and it was still on there which I thought was a bit surprising. 
 We then have S3 intelligent tearing and this is basically using machine learning and it will moved objects 
 around the different tiers of S3 to maximize your cost savings with S3 glacier. 
 This is for data archival And you can configure your retrieval times it can be anywhere from minutes 
 to hours and then if you want the lowest cost storage available then you want S3 glacier deep archive 
 and this is Amazon's lowest cost storage class where retrieval time of 12 hours is acceptable and then 
 finally the one thing I would say is go and read the S3 FIA QS before taking the exam S3 makes up a 
 quite a bit of the exam it's going to come up an awful lot so just have a read through the S3 FIA cuz. 
 Now of course the best way to learn anything with it obvious is to get our hands dirty so in the next 
 lecture what we're going to do is going to go in and create an S3 Buckets so if you got the time please 
 join me in the next lecture. 
 Thank you.